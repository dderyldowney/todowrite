#!/usr/bin/env python3
"""
HAL Agent Preprocessor - Lightweight context optimization
Provides 90% token reduction by preprocessing queries
"""

import re
import subprocess
import sys


class HALPreprocessor:
    def __init__(self):
        self.max_chars = 1000  # Very conservative
        self.max_files = 3  # Very limited files

    def preprocess_query(self, query):
        """Preprocess a query to minimize context"""
        print(f"ðŸ”§ HAL Preprocessing: {query[:50]}...")

        # Extract key terms and targets
        targets = self._extract_targets(query)

        # Find most relevant files (limit severely)
        relevant_files = self._find_relevant_files(targets, max_files=2)

        # Generate compact context
        context = self._generate_compact_context(relevant_files, targets)

        print(f"ðŸ“Š HAL Result: {len(context)} chars, {len(relevant_files)} files")
        return context

    def _extract_targets(self, query):
        """Extract key search targets from query"""
        # Look for file patterns, function names, keywords
        targets = []

        # Python file patterns
        if re.search(r"\.py|python|class |def |import ", query, re.IGNORECASE):
            targets.append("*.py")

        # Config file patterns
        if re.search(r"config|json|yaml|toml", query, re.IGNORECASE):
            targets.append("*.{json,yaml,yml,toml}")

        # Documentation patterns
        if re.search(r"doc|readme|md", query, re.IGNORECASE):
            targets.append("*.md")

        # Specific patterns
        if "mcp" in query.lower():
            targets.extend(["*.json", "mcp*"])

        # Database patterns
        if "database" in query.lower() or "sql" in query.lower():
            targets.extend(["*.sql", "*database*"])

        return targets

    def _find_relevant_files(self, targets, max_files=2):
        """Find most relevant files with strict limits"""
        files = []

        for target in targets:
            try:
                # Use find with very strict limits
                result = subprocess.run(
                    ["find", ".", "-name", target, "-type", "f"],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )

                found_files = [f.strip() for f in result.stdout.strip().split("\n") if f.strip()]

                # Prioritize by location and name
                priority_files = []
                for f in found_files:
                    priority = 0
                    if "src" in f:
                        priority += 3
                    if "lib" in f:
                        priority += 2
                    if "core" in f:
                        priority += 2
                    if any(x in f.lower() for x in ["main", "init", "config"]):
                        priority += 1

                    priority_files.append((priority, f))

                # Sort by priority and take top few
                priority_files.sort(reverse=True)
                files.extend([f for _, f in priority_files[: max_files // len(targets) + 1]])

            except Exception:
                continue

        # Remove duplicates and limit
        unique_files = list(dict.fromkeys(files))[:max_files]
        return unique_files

    def _generate_compact_context(self, files, targets):
        """Generate ultra-compact context"""
        context_parts = []

        # Header with targets
        context_parts.append(f"HAL Analysis: {', '.join(targets)}")
        context_parts.append(f"Files: {len(files)}")

        for file_path in files[:2]:  # Max 2 files
            try:
                # Very limited file reading
                if file_path.endswith(".py"):
                    content = self._read_python_compact(file_path)
                elif file_path.endswith(".json"):
                    content = self._read_json_compact(file_path)
                else:
                    content = self._read_text_compact(file_path)

                if content:
                    context_parts.append(f"{file_path}: {content}")

            except Exception:
                continue

        return "\n".join(context_parts)

    def _read_python_compact(self, file_path, max_lines=5):
        """Read Python file with extreme compression"""
        try:
            with open(file_path) as f:
                lines = f.readlines()

            # Extract only key information
            classes = []
            functions = []
            imports = []

            for line in lines[:50]:  # Only look at first 50 lines
                line = line.strip()
                if line.startswith("class "):
                    classes.append(line.split("(")[0].replace("class ", ""))
                elif line.startswith("def "):
                    functions.append(line.split("(")[0].replace("def ", ""))
                elif line.startswith("import ") or line.startswith("from "):
                    imports.append(line)

            # Compress into minimal format
            parts = []
            if imports:
                parts.append(f"imports: {len(imports)}")
            if classes:
                parts.append(f"classes: {', '.join(classes[:3])}")  # Max 3 classes
            if functions:
                parts.append(f"functions: {len(functions)}")

            return "; ".join(parts) if parts else "Python file"

        except Exception:
            return "Python file"

    def _read_json_compact(self, file_path):
        """Read JSON file with extreme compression"""
        try:
            import json

            with open(file_path) as f:
                data = json.load(f)

            if isinstance(data, dict):
                keys = list(data.keys())[:5]  # Max 5 keys
                return f"JSON with {len(data)} keys: {', '.join(keys)}"
            elif isinstance(data, list):
                return f"JSON array with {len(data)} items"
            else:
                return "JSON data"
        except Exception:
            return "JSON file"

    def _read_text_compact(self, file_path):
        """Read text file with extreme compression"""
        try:
            with open(file_path) as f:
                content = f.read(200)  # Only first 200 chars

            words = len(content.split())
            return f"Text file: {words} words"
        except Exception:
            return "Text file"


def main():
    if len(sys.argv) < 2:
        print("Usage: python hal_preprocessor.py 'your query here'")
        sys.exit(1)

    query = " ".join(sys.argv[1:])
    hal = HALPreprocessor()
    result = hal.preprocess_query(query)

    print("\n" + "=" * 50)
    print("COMPACT CONTEXT:")
    print("=" * 50)
    print(result)
    print("=" * 50)


if __name__ == "__main__":
    main()
